{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Introduction to Algorithms, 4th Edition: Part II Sorting and Order Statistics - Chapter 7 Quicksort - Section 7.2 Performance of Quicksort",
   "id": "2096460d68234c7b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The running time of quicksort depends on how balanced each partitioning is, which in turn depends on which elements are used as pivots. If the two sides of a partition are about the same size - the partitioning is balanced - then the algorithm runs asymptotically as fast as merge sort. If the partitioning is unbalanced, however, it can run asymptotically as slowly as insertion sort. To allow you to gain some intuition before diving into a formal analysis, this section informally investigates how quicksort performs under the assumptions of balanced versus unbalanced partitioning.\n",
    "\n",
    "But first, let's briefly look at the maximum amount of memory that quicksort requires. Although quicksort sorts in place according to the given definition, the amount of memory it uses - aside from the array being sorted - is not constant. Since each recursive call requires a constant amount of space on the runtime stack, outside the array being sorted, quicksort requires space proportional to the maximum depth of the recursion. As we'll see now, that could be as bad as $\\Theta(n)$ in the worst case."
   ],
   "id": "5badf35cbb3595da"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Worst-Case Partitioning\n",
    "The worst-case behavior for quicksort occurs when the partitioning produces one subproblem with $n - 1$ elements and one with $0$ elements. (See Section 7.4.1.) Let us assume that this unbalanced partitioning arises in each recursive call. The partitioning costs $\\Theta(n)$ time. Since the recursive call on an array of size $0$ just returns without doing anything, $T(0) = \\Theta(1)$, and the recurrence for the running time is\n",
    "\n",
    "$$\n",
    "T(n) = T(n - 1) + T(0) + \\Theta(n) = T(n - 1) + \\Theta(n).\n",
    "$$\n",
    "\n",
    "By summing the costs incurred at each level of the recursion, we obtain an arithmetic series (equation (A.3) in Appendix A), which evaluates to $\\Theta\\left(n^2\\right)$. Indeed, the substitution method can be used to prove that the recurrence $T(n) = T(n - 1) + \\Theta(n)$ has the solution $T(n) = \\Theta\\left(n^2\\right)$\n",
    "\n",
    "Thus, if the partitioning is maximally unbalanced at every recursive level of the algorithm, the running time is $\\Theta\\left(n^2\\right)$. The worst-case running time of quicksort is therefore no better than that of insertion sort. Moreover, the $\\Theta\\left(n^2\\right)$ running time occurs when the input array is already completely sorted - a situation in which insertion sort runs in $O(n)$ time."
   ],
   "id": "d1b832f7c089e14a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Best-Case Partitioning\n",
    "In the most even split possible, $\\texttt{Partition}$ produces two subproblems each of size no more than $\\frac{n}{2}$, since one is of size $\\left\\lfloor\\frac{n - 1}{2}\\right\\rfloor \\leq \\frac{n}{2}$ and one of size $\\left\\lceil\\frac{n - 1}{2}\\right\\rceil - 1 \\leq \\frac{n}{2}$. In this case, quicksort runs much faster. An upper bound on the running time can then be described by the recurrence\n",
    "\n",
    "$$\n",
    "T(n) = 2T\\left(\\frac{n}{2}\\right) + \\Theta(n).\n",
    "$$\n",
    "\n",
    "By case 2 of the master theorem (Theorem 4.1), this recurrence has the solution $T(n) = \\Theta(n\\lg{n})$. Thus, if the partitioning is equally balanced at every level of recursion, an asymptotically faster algorithm results."
   ],
   "id": "143eda3c1d6fe09d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Balanced Partitioning\n",
    "As the analyses in Section 7.4 will show, the average-case running time of quicksort is much closer to the best case than to the worst case. By appreciating how the balance of the partitioning affects the recurrence describing the running time, we can gain an understanding of why.\n",
    "\n",
    "Suppose, for example, that the partitioning algorithm always produces a $9$-to-$1$ proportional split, which at first blush seems quite unbalanced. We then obtain the recurrence\n",
    "\n",
    "$$\n",
    "T(n) = T\\left(\\frac{9n}{10}\\right) + T\\left(\\frac{n}{10}\\right) + \\Theta(n),\n",
    "$$\n",
    "\n",
    "on the running time of quicksort. Figure 7.4 shows the recursion tree for this recurrence, where for simplicity the $\\Theta(n)$ driving function has been replaced by $n$, which won't affect the asymptotic solution of the recurrence. Every level of the tree has cost $n$, until the recursion bottoms out in a base case at depth $\\log_{10}{n} = \\Theta(\\lg{n})$, and then the levels have cost at most $n$. The recursion terminates at depth $\\log_\\frac{10}{9}{n} = \\Theta(\\lg{n})$. Thus, with a $9$-to-$1$ proportional split at every level of recursion, which intuitively seems highly unbalanced, quicksort runs in $O(n\\lg{n})$ time - asymptotically the same as if the split were right down the middle. Indeed, even a $99$-to-$1$ split yields an $O(n\\lg{n})$ running time. In fact, any split of *constant* proportionality yields a recursion tree of depth $\\Theta(\\lg{n})$, where the cost at each level is $O(n)$. The running time is therefore $O(n\\lg{n})$ whenever the split has constant proportionality. The ratio of the split affects only the constant hidden in the $O$-notation.\n",
    "\n",
    "<img src=\"Figure 7.4.png\" alt=\"Figure 7.4\" width=\"500\"/>"
   ],
   "id": "be076fc7a93c5eec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Intuition for the Average Case\n",
    "To develop a clear notion of quicksort's expected behavior, we must assume something about how its inputs are distributed. Because quicksort determines the sorted order using only comparisons between input elements, its behavior depends on the relative ordering of the values in the array elements given as the input, not on the particular values in the array. As in the probabilistic analysis of the hiring problem in Section 5.2, assume that all permutations of the input numbers are equally likely and that the elements are distinct.\n",
    "\n",
    "When quicksort runs on a random input array, the partitioning is highly unlikely to happen in the same way at every level, as our informal analysis has assumed. We expect that some splits will be reasonably well-balanced and that some will be fairly unbalanced.\n",
    "\n",
    "In the average case, $\\texttt{Partition}$ produces a mix of \"good\" and \"bad\" splits. In a recursion tree for an average-case execution of $\\texttt{Partition}$, the good and bad splits are distributed randomly throughout the tree. Suppose for the sake of intuition that the good and bad splits alternate levels in the tree, and that the good splits are best-case splits and the bad splits are worst-case splits. Figure 7.5(a) shows the splits at two consecutive levels in the recursion tree. At the root of the tree, the cost is $n$ for partitioning, and the subarrays produced have sizes $n - 1$ and $0$: the worst case. At the next level, the subarray of size $n - 1$ undergoes best-case partitioning into subarrays of size $\\frac{n - 1}{2} - 1$ and $\\frac{n - 1}{2}$. Let us assume that the best-case cost is $1$ for the subarray of size $0$.\n",
    "\n",
    "The combination of the bad split followed by the good split produces three subarrays of size $0$, $\\frac{n - 1}{2} - 1$, and $\\frac{n - 1}{2}$ at a combined partitioning cost of $\\Theta(n) + \\Theta(n - 1) = \\Theta(n)$ This situation is at most a constant factor worse than that in Figure 7.5(b), namely, where a single level of partitioning produces two subarrays of size $\\frac{n - 1}{2}$, at a cost of $\\Theta(n)$. Yet this latter situation is balanced! Intuitively, the $\\Theta(n - 1)$ cost of the bad split in Figure 7.5(a) can be absorbed into the $\\Theta(n)$ cost of the good split, and the resulting split is good. Thus, the running time of quicksort, when levels alternate between good and bad splits, is like the running time for goood splits alone: still $O(n\\lg{n})$, but with a slightly larger constant hidden by the $O$-notation. We'll analyze the expected running time of a randomized version of quicksort rigorously in Section 7.4.2."
   ],
   "id": "b26769482acd208d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
