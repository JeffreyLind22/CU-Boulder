{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Introduction to Algorithms, 4th Edition: Part I Foundations - Chapter 2 Getting Started - Section 2.3 Designing Algorithms",
   "id": "3a6a8d9f2bfef717"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "You can choose from a wide range of algorithm design techniques. Insertion sort uses the incremental method: for each element $A[i]$, insert it into its proper place in the subarray $A[1:i]$, having already sorted the subarray $A[1:i-1]$.\n",
    "\n",
    "This section examines another design method, known as \"divide-and-conquer.\""
   ],
   "id": "ad3368da512317c2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## The Divide-and-Conquer Method\n",
    "Many useful algorithms are **recursive** in structure: to solve a given problem, they **recurse** (call themselves) one or more times to handle closely related subproblems. These algorithms typically follow the **divide-and-conquer** method: they break the problem into several subproblems that are similar to the original problem but smaller, solve the subproblems recursively, and then combine these solutions to create a solution to the original problem.\n",
    "\n",
    "In the divide-and-conquer method, if the problem is small enough - the **base case** - you just solve it directly without recursing. Otherwise - the **recursive case** - you perform three characteristic steps:\n",
    "* **Divide** the problem into one or more subproblems that are smaller instances of the same problem.\n",
    "* **Conquer** the subproblems by solving them recursively.\n",
    "* **Combine** the subproblem solutions to form a solution to the original problem."
   ],
   "id": "b9d4e59cbf15e5c8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Merge Sort\n",
    "The merge sort algorithm closely follows the divide-and-conquer method. In each step, it sorts a subarray $A[p:r]$, starting with the entire array $A[1:n]$ and recursing down to smaller and smaller subarrays. Here is how merge sort operations:\n",
    "\n",
    "* **Divide** the subarray $A[p:r]$ to be sorted into two adjacent subarrays, each of half the size. To do so, compute the midpoint $q$ of $A[p:r]$ (taking the average of $p$ and $r$), and divide $A[p:r]$ into subarrays $A[p:q]$ and $A[q+1:r]$\n",
    "* **Conquer** by sorting each of the two subarrays $A[p:q]$ and $A[q+1:r]$ recursively using merge sort.\n",
    "* **Combine** by merging the two sorted subarrays $A[p:q]$ and $A[q+1:r]$ back into $A[p:r]$, producing the sorted answer.\n",
    "\n",
    "The recursion \"bottoms out\" - it reaches the base case - when the subarray $A[p:r]$ to be sorted has just 1 element, that is, when $p=r$. As we noted in the initialization argument of $\\textsc{\\text{Insertion-Sort}}$'s loop invariant, a subarray with cardinality 1 is always sorted.\n",
    "\n",
    "The key operation of the merge sort algorithm occurs in the \"combine\" step, which merges two adjacent, sorted subarrays. The merge operation is performed by the auxiliary procedure $\\texttt{Merge(A, p, q, r)}$, where $A$ is an array and $p$, $q$, and $r$ are indices into the array such that $p \\leq q < r$. The procedure assumes that the adjacent subarrays $A[p:q]$ and $A[q+1:r]$ were already recursively sorted. It **merges** the two sorted subarrays to form a single sorted subarray that replaces the current subarray $A[p:r]$.\n",
    "\n",
    "Taking the card analogy, let's consider how long it takes to merge two sorted piles of cards. Each basic step takes constant time, since you are comparing just the two top cards. If the two sorted piles that you start with each have $\\frac{n}{2}$ cards, then the number of basic steps is at least $\\frac{n}{2}$ (since in whichever piles was emptied, every card was found to be smaller than some card from the other pile) and at most $n$ (actually, at most $n-1$, since after $n-1$ basic steps, one of the piles must be emptied). With each basic step taking constant time and the total number of basic steps being between $\\frac{n}{2}$ and $n$, we can say that merging takes time roughly proportional to $n$. That is, merging takes $\\Theta\\left(n\\right)$ time.\n",
    "\n",
    "In detail, the $\\textsc{\\text{Merge}}$ procedure works as follows:\n",
    "```\n",
    "Merge(A, p, q, r)\n",
    "[1]  n_L = q - p + 1 // Length of A[p:q]\n",
    "[2]  n_R = r - q // Length of A[q+1:r]\n",
    "[3]  let L[0:n_L-1\\ and R[0:n_R-1] be new arrays\n",
    "[4]  for i=0 to n_L-1 // Copy A[p:q] into L[0:n_L-1]\n",
    "[5]      L[i] = A[p+i]\n",
    "[6]  for j=0 to n_R-1 // Copy A[q+1:r] into R[0:n_R-1]\n",
    "[7]      R[j] = A[q+j+1]\n",
    "[8]  i=0 // i indexes the smallest element remaining in L\n",
    "[9]  j=0 // j indexes the smallest element remaining in R\n",
    "[10] k=p // k indexes the location in A to fill\n",
    "[11] // As long as each of the arrays L and R contains an unmerged element, copy the smallest unmerged element back into A[p:r].\n",
    "[12] while i<n_L and j<n_R\n",
    "[13]     if L[i] <= R[j]\n",
    "[14]         A[k] = L[i]\n",
    "[15]         i = i + 1\n",
    "[16]     else A[k] = R[j]\n",
    "[17]          j = j + 1\n",
    "[18]          k = k + 1\n",
    "[19] // Having gone through one of L and R entirely, copy the remainder of the other to the end of A[p:r].\n",
    "[20] while i < n_L\n",
    "[21]     A[k] = L[i]\n",
    "[22]     i = i + 1\n",
    "[23]     k = k + 1\n",
    "[24] while j < n_R\n",
    "[25]     A[k] = R[i]\n",
    "[26]     j = j + 1\n",
    "[27]     k = k + 1\n",
    "```\n",
    "It copies the two subarrays $A[p:q]$ and $A[q+1:r]$ into temporary arrays $L$ and $R$ (\"left\" and \"right\"), and then it merges the values of $L$ and $R$ back into $A[p:r]$. Lines 1 and 2 compute the lengths $n_L$ and $n_R$ of the subarrays $A[p:q]$ and $A[q+1:r]$, respectively. Then, line 3 creates arrays $L[0:n_L-1]$ and $R[0:n_R-1]$ with respective lengths $n_L$ and $n_R$. The **for** loop of lines 4-5 copies the subarray $A[p:q]$ into $L$, and the **for** loop of lines 6-7 copies the subarray $A[q+1:r]$ into $R$.\n",
    "\n",
    "Lines 8-18 perform the basic steps. The **while** loop of lines 12-18 repeatedly identifies the smallest value in $L$ and $R$ that has yet to be copied back into $A[p:r]$ and copies it back in. As the comments indicate, the index $k$ gives the position of $A$ that is being filled in, and the indices $i$ and $j$ give the positions of $L$ and $R$, respectively, of the smallest remaining values. Eventually, all of $L$ or all of $R$ is copied back into $A[p:r]$, and this loop terminates. If the loop terminates because all of $R$ has been copied back, that is, because $j = n_R$, then $i < n_L$, so that some of $L$ has yet to be copied back, and these values are the greatest in both $L$ and $R$. In this case, the **while** loop of lines 20-23 copies these remaining values of $L$ into the last few positions of $A[p:r]$. Because $j$ equals $n_R$, the **while** loop of lines 24-27 iterates 0 times. If instead the **while** loop of lines 12-18 terminates because $i=n_L$, then all of $L$ has already been copied back into $A[p:r]$, and the **while** loop of lines 24-27 copies the remaining values of $R$ back into the end of $A[p:r]$.\n",
    "\n",
    "To see that the $\\textsc{\\text{Merge}}$ procedure runs in $\\Theta\\left(n\\right)$ time, where $n = r - p + 1$, observe that each of lines 1-3 and 8-10 takes constant time, and the **for** loops of lines 4-7 take $\\Theta\\left(n_L + n_R\\right) = \\Theta\\left(n\\right)$ time. To account for the three **while** loops of lines 12-18, 20-23, and 24-27, observe that each iteration of these loops copies exactly one value from $L$ or $R$ back to $A$ and that every value is copied back into $A$ exactly once. Therefore, these three loops together make a total of $n$ iterations. Since each iteration of each of the three loops takes constant time, the total time spent in these three loops is $\\Theta\\left(n\\right)$.\n",
    "\n",
    "We can now use the $\\textsc{\\text{Merge}}$ procedure as a subroutine in the merge sort algorithm. The procedure $\\texttt{Merge-Sort(A, p, r)}$ sorts the elements in the subarray $A[p:r]$. If $p=r$, the subarray has just 1 element and is therefore already sorted. Otherwise, we have $p < r$, and $\\texttt{Merge-Sort}$ runs the divide, conquer, and combine steps. The divide step simply computes an index $q$ that partitions $A[p:r]$ into two adjacent subarrays: $A[p:q]$, containing $\\left\\lceil\\frac{n}{2}\\right\\rceil$ elements, and $A[q+1:r]$, containing $\\left\\lfloor\\frac{n}{2}\\right\\rfloor$ elements. The initial call $\\texttt{Merge-Sort(A, 1, rn)}$ sorts the entire array $A[1:n]$.\n",
    "\n",
    "```\n",
    "Merge-Sort(A, p, r)\n",
    "    if p >= r // Zero or one element?\n",
    "        return\n",
    "    q = floor((p + r) / 2)) // Midpoint of A[p : r]\n",
    "    Merge-Sort(A, p, q) // Recursively sort A[p : q]\n",
    "    Merge-Sort(A, q + 1, r) // Recursively sort A[q + 1 : r]\n",
    "    // Merge A[p:q] and A[q + 1 : r] into A[p : r].\n",
    "    Merge(A, p, q, r)\n",
    "```"
   ],
   "id": "64cfac599f9d30f5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Analyzing Divide-and-Conquer Algorithms\n",
    "When an algorithm contains a recursive call, you can often describe its running time by a **recurrence equation** or **recurrence**, which describes the overall running time on a problem of size $n$ in terms of the running time of the same algorithm on smaller inputs. You can then use mathematical tools to solve the recurrence and provide bounds on the performance of the algorithm.\n",
    "\n",
    "A recurrence for the running time of a divide-and-conquer algorithm falls out from the three steps of the basic method. As we did for insertion sort, let $T\\left(n\\right)$ be the worst-case running time on a problem of size $n$. If the problem size is small enough, say $n < n_0$ for some constant $n_0 > 0$, the straightforward solution takes constant time, which we write as $\\Theta\\left(1\\right)$. Suppose that the division of the problem yields $a$ subproblems, each with size $\\frac{n}{b}$, that is, $\\frac{1}{b}$ the size of the original. For merge sort, $a = b = 2$, but we'll see other divide-and-conquer algorithms in which $a \\neq b$. It takes $T\\left(\\frac{n}{b}\\right)$ time to solve one subproblem of size $\\frac{n}{b}$, and so it takes $aT\\left(\\frac{n}{b}\\right)$ time to solve all $a$ of them. If it takes $D\\left(n\\right)$ time to divide the problem into subproblems and $C\\left(n\\right)$ time to combine the solutions to the subproblems into the solution to the original problem, we get the recurrence\n",
    "$$\n",
    "T\\left(n\\right) = \\begin{cases} \n",
    "                      \\Theta\\left(1\\right) & \\quad n < n_0 \\\\\n",
    "                      D\\left(n\\right) + aT\\left(\\frac{n}{b}\\right) + C\\left(n\\right) & \\quad \\text{otherwise}\n",
    "                   \\end{cases}\n",
    "$$\n",
    "Sometimes, the $\\frac{n}{b}$ size of the divide step isn't an integer. For example, the $\\texttt{Merge-Sort}$ procedure divides a problem of size $n$ into subproblems of sizes $\\left\\lceil\\frac{n}{2}\\right\\rceil$ and $\\left\\lfloor\\frac{n}{2}\\right\\rfloor$. Since the difference between $\\left\\lceil\\frac{n}{2}\\right\\rceil$ and $\\left\\lfloor\\frac{n}{2}\\right\\rfloor$ is at most 1, which for large $n$ is much smaller than the effect of dividing $n$ by 2, we'll make the assumption and just call them both size $\\frac{n}{2}$."
   ],
   "id": "819d7d4c9e2d4290"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Analysis of Merge Sort\n",
    "Here's how to set up the recurrence for $T\\left(n\\right)$, the worst-case running time of merge sort on $n$ numbers.\n",
    "\n",
    "* **Divide:** The divide step just computes the middle of the subarray, which takes constant time. Thus, $D\\left(n\\right) = \\Theta\\left(1\\right)$\n",
    "* **Conquer:** Recursively solving two subproblems, each of size $\\frac{n}{2}$, contributes $2T\\left(\\frac{n}{2}\\right)$ to the running time (ignoring the floors and ceilings, as we discussed).\n",
    "* **Combine:** Since the $\\texttt{Merge}$ procedure on an $n$-element array takes $\\Theta\\left(n\\right)$ time, we have $C\\left(n\\right) = \\Theta\\left(n\\right)$\n",
    "\n",
    "When we add the functions $D\\left(n\\right)$ and $C\\left(n\\right)$ for the merge sort analysis, we are adding a function that is $\\Theta\\left(n\\right)$ and a function that is $\\Theta\\left(1\\right)$. The sum is a linear function of $n$. That is, it is roughly proportional to $n$ when $n$ is large, and so merge sort's dividing and combining times together are $\\Theta\\left(n\\right)$. Adding $\\Theta\\left(n\\right)$ to the $2T\\left(\\frac{n}{2}\\right)$ term from the conquer step gives the recurrence for the worst-case running time $T\\left(n\\right)$ of merge sort:\n",
    "$$\n",
    "\\begin{equation}\n",
    "T\\left(n\\right) = 2T\\left(\\frac{n}{2}\\right) + \\Theta\\left(n\\right) \\quad \\mathbf{\\left(2.3\\right)}\n",
    "\\end{equation}\n",
    "$$\n",
    "We do not need the master theorem from Chapter 4, however, to understand intuitively why the solution to recurrence (2.3) is $T(n) = \\Theta\\left(n\\lg{n}\\right)$. For simplicity, assume that $n$ is an exact power of 2 and that the implicit base case is $n = 1$. Then recurrence (2.3) is essentially\n",
    "$$\n",
    "T\\left(n\\right) = \\begin{cases} \n",
    "                      c_1 & \\quad \\text{if} \\ n = 1. \\\\\n",
    "                      2T\\left(\\frac{n}{2}\\right) + c_2 n & \\quad \\text{if} \\ n > 1.\n",
    "                   \\end{cases} \\quad \\textbf{(2.4)}\n",
    "$$\n",
    "where the constant $c_1 > 0$ represents the time required to solve a problem of size 1, and $c_2 > 0$ is the time per array element of the divide and combine steps.\n",
    "\n",
    "The figure below illustrates one way of figuring out the solution to recurrence (2.4).\n",
    "\n",
    "<img src=\"Recursion Tree.png\" alt=\"Recursion Tree\" width=\"500\"/>\n",
    "\n",
    "Part (a) of the figure shows $T\\left(n\\right)$, which part (b) expands into an equivalent tree representing the recurrence. The $c_2 n$ term denotes the cost of dividing and combining at the top level of recursion, and the two subtrees of the root are the two smaller recurrences $T\\left(\\frac{n}{2}\\right)$. Part (c) shows this process carried one step further by expanding $T\\left(\\frac{n}{2}\\right)$. The cost for dividing and combining at each of the two nodes at the second level of recursion is \\frac{c_2 n}{2}. Continue to expand each node in the tree by breaking it into its constituent parts as determined by the recurrence, until the problem sizes get down to 1, each with a cost of $c_1$. Part (d) shows the resulting **recursion tree**.\n",
    "\n",
    "Next, add the costs across each level of the tree. The top level has total cost $c_2 n$, the next level down has $c_2 n$, the level after that has total cost $c_2 n$, and so on. Each level has twice as many nodes as the level above, but each node contributes only half the cost of a node from the level above. From one level to the next, doubling and halving cancel each other out, so that the cost across each level is the same: $c_2 n$. In general, the level that is $i$ levels below the top has $2^i$ nodes, each contributing a cost of $c_2 \\left(\\frac{n}{2^i}\\right)$, so that the $i$th level below the top has total cost $2^i c_2 \\left(\\frac{n}{2^i}\\right) = c_2 n$. The bottom level has $n$ nodes, each contributing a cost of $c_1$, for a total cost of $c_1 n$.\n",
    "\n",
    "The total number of levels of the recursion tree is $\\lg{n} + 1$, where $n$ is the number of leaves, corresponding to the input size. An informal inductive argument justifies this claim. The base case occurs when $n = 1$, in which case the tree has only 1 level. Since $\\lg{1} = 0$, we have that $\\lg{n} + 1$ gives the correct number of levels. Now assume as an inductive hypothesis that the number of levels of a recursion tree with $2^i$ leaves is $\\lg{2^i} + 1 = i + 1$ (since for any value of $i$, we have that $\\lg{2^i} = i$). Because we assume that the input size is an exact power of 2, the next input size to consider is $2^{i + 1}$. A tree with $n = 2^{i + 1}$ leaves has 1 more level than a tree with $2^i$ leaves, and so the total number of levels is $(i + 1) + 1 = \\lg{2^{i + 1}} + 1$.\n",
    "\n",
    "To compute the total cost represented by the recurrence (2.4), simply add up the costs of all the levels. The recursion tree has $\\lg{n} = 1$ levels. The levels above the leaves each cost $c_2 n$, and the leaf level costs $c_1 n$, for a total cost of $c_2 n \\lg{n} + c_1 n = \\Theta\\left(n\\lg{n}\\right)$."
   ],
   "id": "2a06b47082d291cd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Exercises",
   "id": "a37d235f7f9bf4d6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.3-1\n",
    "**Using the Recursion Tree figure as a model, illustrate the operation of merge sort on an array initially containing the sequence $\\langle 3, 41, 52, 26, 38, 57, 9, 49 \\rangle$.**"
   ],
   "id": "87010a2d16e82ab3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
